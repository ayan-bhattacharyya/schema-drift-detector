identify_sources:
  description: >
    Query the GraphDB integration/catalog to resolve authoritative source(s), entity mappings,
    and policy metadata for the pipeline identifier.
  expected_output: |
    {
      "pipeline": "<pipeline_name_or_id>",
      "sources": [
        { "source_id": "<id>", "type": "<db|api|csv>", "entity": "<entity_name>", "metadata_ref": "<neo4j_node_id_or_props>" },
        ...
      ],
      "policies": { "auto_heal_allowed": <bool>, "notify_rules": { ... } }
    }
  agent: source_schema_identifier

crawl_database:
  description: >
    Check the 'sources' list from the 'identify_sources' task output.
    IF there is a source with type 'db' (or 'database'):
      Use DB system catalogs / information_schema to build a canonical schema snapshot
      for the named table/entity. Must not read data rows or sample values.
    ELSE:
      Return "Skipped: Source type is not database."
  expected_output: |
    {
      "request_id": "<uuid>",
      "source_id": "<source_id>",
      "entity": "<table_name>",
      "snapshot": {
        "source_id": "<source_id>",
        "entity": "<table_name>",
        "schema": {
          "fields": [
            { "name": "<col>", "type": "<type>", "nullable": <bool>, "constraints": { ... } },
            ...
          ],
          "version_meta": { "created_by":"database_crawler_agent", "timestamp":"<ts>" }
        }
      }
    }
  agent: database_crawler_agent

crawl_api:
  description: >
    Check the 'sources' list from the 'identify_sources' task output.
    IF there is a source with type 'api':
      Parse OpenAPI/Swagger/GraphQL/RAML specs (from contract ref in metadata) and return a canonical contract snapshot.
      Must not invoke endpoints to sample responses.
    ELSE:
      Return "Skipped: Source type is not api."
  expected_output: |
    {
      "request_id": "<uuid>",
      "source_id": "<source_id>",
      "entity": "<resource_or_schema_name>",
      "snapshot": {
        "source_id": "<source_id>",
        "entity": "<resource_or_schema_name>",
        "schema": {
          "endpoints": [ ... ],
          "models": { "<model_name>": { "fields": [ { "name":"<f>", "type":"<t>" }, ... ] } },
          "version_meta": { "created_by":"api_crawler_agent", "timestamp":"<ts>" }
        }
      }
    }
  agent: api_crawler_agent

crawl_csv:
  description: >
    Check the 'sources' list from the 'identify_sources' task output.
    IF there is a source with type 'csv' (or 'file' or 'text'):
      Inspect CSV/flat-file headers and object metadata (not contents) to build a canonical schema snapshot.
      Must not read or persist data rows or sample values.
    ELSE:
      Return "Skipped: Source type is not csv/file."
  expected_output: |
    {
      "request_id": "<uuid>",
      "source_id": "<source_id>",
      "entity": "<logical_name>",
      "snapshot": {
        "source_id": "<source_id>",
        "entity": "<logical_name>",
        "schema": {
          "fields": [
            { "name":"<col>", "type":"<inferred_type|string>", "nullable": <bool>, "hints": { ... } },
            ...
          ],
          "version_meta": { "created_by":"csv_crawler_agent", "timestamp":"<ts>" }
        }
      }
    }
  agent: csv_crawler_agent

persist_snapshots:
  description: >
    Persist one or more canonical snapshots into the GraphDB as a versioned snapshot.
    Validate snapshots to ensure they contain only metadata (no PII or sample values).
    CRITICAL: If the storage operation fails, set 'stored': false in the output.
    The workflow MUST raise an error if 'stored' is false.
  expected_output: |
    {
      "request_id": "<uuid>",
      "snapshot_ids": [ "<snapshot_id1>", "<snapshot_id2>", ... ],
      "stored": <bool>,
      "stored_by": "metadata_agent"
    }
  agent: metadata_agent

detect_drift:
  description: >
    Compare the newly stored snapshot(s) against previous snapshot(s) and produce a drift_report
    describing changes, severity, and short summary. Uses rules-based logic and optional LLM
    augmentation (on metadata-only content).
  expected_output: |
    {
      "request_id": "<uuid>",
      "drift_detected": <bool>,
      "drift_report": {
        "changes": [
          { "op": "<add|remove|change>", "field": "<name>", "before": {...}, "after": {...}, "severity": "<low|medium|high>" },
          ...
        ],
        "summary": "<human friendly summary>",
        "severity": "<info|warning|critical>"
      },
      "detected_by": "detector_agent"
    }
  agent: detector_agent

generate_healing:
  description: >
    CONDITIONAL: Check the 'drift_detected' field from the 'detect_drift' task output.
    IF 'drift_detected' is true:
      Generate healing actions: SQL/dbt patches, API contract updates, or migration hints.
      Return scripts with confidence based on the drift_report and impacted_lineage.
    ELSE:
      Return {"request_id": "<uuid>", "healing": {"recommended_actions": [], "next_steps": "none"}, "skipped": true, "reason": "No drift detected"}
  expected_output: |
    {
      "request_id": "<uuid>",
      "healing": {
        "recommended_actions": [
          { "type":"<sql|dbt|api_patch>", "script":"<script_text>", "confidence": <0-100> },
          ...
        ],
        "next_steps": "<manual|auto|pause_pipeline|none>"
      },
      "skipped": <bool>,
      "generated_by": "healer_agent"
    }
  agent: healer_agent

notify_operator:
  description: >
    CONDITIONAL: Check TWO conditions from previous task outputs:
    1. Check 'policies.notify_on_breaking' from the 'identify_sources' task output
    2. Check 'drift_detected' from the 'detect_drift' task output
    
    IF both 'policies.notify_on_breaking' is true AND 'drift_detected' is true:
      Send a notification to operators according to policy (email, Slack, Teams, webhook).
      Notification must not contain PII or sample values; include safe links/IDs to console.
    ELSE:
      Return {"request_id": "<uuid>", "skipped": true, "reason": "Notification not required (notify_on_breaking=false or no drift detected)", "sent": false}
  expected_output: |
    {
      "request_id": "<uuid>",
      "notification_id": "<id>",
      "channels": [ "<email|slack|teams|webhook>" ],
      "sent": <bool>,
      "skipped": <bool>,
      "operator_response": { "decision": "<approve|reject|defer>|null", "timestamp": "<ts>|null" }
    }
  agent: notification_agent

finalize_decision:
  description: >
    Aggregate outputs from detector/healer/notification and produce the final decision returned to the caller.
    Decides whether the pipeline should continue, pause, or require manual review.
  expected_output: |
    {
      "request_id": "<uuid>",
      "decision": "<continue|pause|manual_review|auto_heal>",
      "details": { "drift": <bool>, "severity": "<info|warning|critical|null>", "snapshot_ids":[...], "healing": {...} }
    }
  agent: orchestrator_agent
